{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4eac794",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Dropout, Activation\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SGD, Adam\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fe00b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build Vocabulary\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a3ac18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Obtaining dependency information for tensorflow from https://files.pythonhosted.org/packages/80/6f/57d36f6507e432d7fc1956b2e9e8530c5c2d2bfcd8821bcbfae271cd6688/tensorflow-2.14.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading tensorflow-2.14.0-cp311-cp311-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting tensorflow-intel==2.14.0 (from tensorflow)\n",
      "  Obtaining dependency information for tensorflow-intel==2.14.0 from https://files.pythonhosted.org/packages/ad/6e/1bfe367855dd87467564f7bf9fa14f3b17889988e79598bc37bf18f5ffb6/tensorflow_intel-2.14.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading tensorflow_intel-2.14.0-cp311-cp311-win_amd64.whl.metadata (4.8 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for absl-py>=1.0.0 from https://files.pythonhosted.org/packages/01/e4/dc0a1dcc4e74e08d7abedab278c795eef54a224363bb18f5692f416d834f/absl_py-2.0.0-py3-none-any.whl.metadata\n",
      "  Downloading absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for flatbuffers>=23.5.26 from https://files.pythonhosted.org/packages/6f/12/d5c79ee252793ffe845d58a913197bfa02ae9a0b5c9bc3dc4b58d477b9e7/flatbuffers-23.5.26-py2.py3-none-any.whl.metadata\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (3.9.0)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for libclang>=13.0.0 from https://files.pythonhosted.org/packages/02/8c/dc970bc00867fe290e8c8a7befa1635af716a9ebdfe3fb9dce0ca4b522ce/libclang-16.0.6-py2.py3-none-win_amd64.whl.metadata\n",
      "  Downloading libclang-16.0.6-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting ml-dtypes==0.2.0 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for ml-dtypes==0.2.0 from https://files.pythonhosted.org/packages/08/89/c727fde1a3d12586e0b8c01abf53754707d76beaa9987640e70807d4545f/ml_dtypes-0.2.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading ml_dtypes-0.2.0-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.23.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (1.24.3)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (23.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 from https://files.pythonhosted.org/packages/c2/59/f89c04923d68595d359f4cd7adbbdf5e5d791257945f8873d88b2fd1f979/protobuf-4.24.4-cp310-abi3-win_amd64.whl.metadata\n",
      "  Downloading protobuf-4.24.4-cp310-abi3-win_amd64.whl.metadata (540 bytes)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow) (1.14.1)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     ------- -------------------------------- 0.3/1.5 MB 5.7 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 1.1/1.5 MB 11.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 11.8 MB/s eta 0:00:00\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for grpcio<2.0,>=1.24.3 from https://files.pythonhosted.org/packages/75/c5/fb3ed7495c73c0de58b08376a468a35bdb61b89ddfbdb96a37bceb54f959/grpcio-1.59.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading grpcio-1.59.0-cp311-cp311-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting tensorboard<2.15,>=2.14 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for tensorboard<2.15,>=2.14 from https://files.pythonhosted.org/packages/73/a2/66ed644f6ed1562e0285fcd959af17670ea313c8f331c46f79ee77187eb9/tensorboard-2.14.1-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for tensorflow-estimator<2.15,>=2.14.0 from https://files.pythonhosted.org/packages/d1/da/4f264c196325bb6e37a6285caec5b12a03def489b57cc1fdac02bb6272cd/tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.15,>=2.14.0 (from tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for keras<2.15,>=2.14.0 from https://files.pythonhosted.org/packages/fe/58/34d4d8f1aa11120c2d36d7ad27d0526164b1a8ae45990a2fede31d0e59bf/keras-2.14.0-py3-none-any.whl.metadata\n",
      "  Downloading keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.14.0->tensorflow) (0.38.4)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/39/7c/2e4fa55a99f83ef9ef229ac5d59c44ceb90e2d0145711590c0fa39669f32/google_auth-2.23.3-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth-2.23.3-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow)\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2.31.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/da/61/6e9ff8258422d287eec718872fb71e05324356722ab658c8afda25f51539/tensorboard_data_server-0.7.1-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2.2.3)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow)\n",
      "  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a9/c9/c8a7710f2cedcb1db9224fdd4d8307c9e48cbddc46c18b515fefc0f1abbe/cachetools-5.3.1-py3-none-any.whl.metadata\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow)\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow)\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     ---------------------------------------- 0.0/151.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 151.7/151.7 kB ? eta 0:00:00\n",
      "Downloading tensorflow-2.14.0-cp311-cp311-win_amd64.whl (2.1 kB)\n",
      "Downloading tensorflow_intel-2.14.0-cp311-cp311-win_amd64.whl (284.2 MB)\n",
      "   ---------------------------------------- 0.0/284.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/284.2 MB 31.8 MB/s eta 0:00:09\n",
      "   ---------------------------------------- 3.0/284.2 MB 32.1 MB/s eta 0:00:09\n",
      "    --------------------------------------- 4.7/284.2 MB 33.1 MB/s eta 0:00:09\n",
      "    --------------------------------------- 6.7/284.2 MB 35.4 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 8.5/284.2 MB 38.9 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 10.5/284.2 MB 36.3 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 12.5/284.2 MB 40.9 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 14.4/284.2 MB 43.5 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 16.5/284.2 MB 43.5 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 17.5/284.2 MB 40.9 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 19.4/284.2 MB 40.9 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 20.6/284.2 MB 36.4 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 21.3/284.2 MB 32.8 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 22.0/284.2 MB 29.7 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 22.8/284.2 MB 28.5 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 23.6/284.2 MB 26.2 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 24.2/284.2 MB 24.2 MB/s eta 0:00:11\n",
      "   --- ------------------------------------ 24.8/284.2 MB 22.5 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 25.4/284.2 MB 21.1 MB/s eta 0:00:13\n",
      "   --- ------------------------------------ 26.1/284.2 MB 19.3 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 26.1/284.2 MB 19.3 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 26.6/284.2 MB 16.8 MB/s eta 0:00:16\n",
      "   --- ------------------------------------ 27.1/284.2 MB 16.4 MB/s eta 0:00:16\n",
      "   --- ------------------------------------ 27.7/284.2 MB 15.6 MB/s eta 0:00:17\n",
      "   --- ------------------------------------ 28.0/284.2 MB 15.2 MB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 28.5/284.2 MB 14.2 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 28.9/284.2 MB 13.6 MB/s eta 0:00:19\n",
      "   ---- ----------------------------------- 29.3/284.2 MB 12.8 MB/s eta 0:00:20\n",
      "   ---- ----------------------------------- 29.7/284.2 MB 12.6 MB/s eta 0:00:21\n",
      "   ---- ----------------------------------- 30.1/284.2 MB 11.7 MB/s eta 0:00:22\n",
      "   ---- ----------------------------------- 30.5/284.2 MB 11.5 MB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 30.8/284.2 MB 11.1 MB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 31.2/284.2 MB 10.7 MB/s eta 0:00:24\n",
      "   ---- ----------------------------------- 31.7/284.2 MB 10.6 MB/s eta 0:00:24\n",
      "   ---- ----------------------------------- 32.2/284.2 MB 10.6 MB/s eta 0:00:24\n",
      "   ---- ----------------------------------- 32.7/284.2 MB 10.4 MB/s eta 0:00:25\n",
      "   ---- ----------------------------------- 33.0/284.2 MB 10.1 MB/s eta 0:00:25\n",
      "   ---- ----------------------------------- 33.6/284.2 MB 9.9 MB/s eta 0:00:26\n",
      "   ---- ----------------------------------- 34.2/284.2 MB 9.9 MB/s eta 0:00:26\n",
      "   ---- ----------------------------------- 34.7/284.2 MB 9.8 MB/s eta 0:00:26\n",
      "   ---- ----------------------------------- 35.0/284.2 MB 9.6 MB/s eta 0:00:26\n",
      "   ---- ----------------------------------- 35.3/284.2 MB 9.5 MB/s eta 0:00:27\n",
      "   ----- ---------------------------------- 35.6/284.2 MB 9.2 MB/s eta 0:00:27\n",
      "   ----- ---------------------------------- 35.8/284.2 MB 9.0 MB/s eta 0:00:28\n",
      "   ----- ---------------------------------- 36.1/284.2 MB 8.7 MB/s eta 0:00:29\n",
      "   ----- ---------------------------------- 36.3/284.2 MB 8.6 MB/s eta 0:00:29\n",
      "   ----- ---------------------------------- 36.7/284.2 MB 8.8 MB/s eta 0:00:28\n",
      "   ----- ---------------------------------- 37.1/284.2 MB 8.7 MB/s eta 0:00:29\n",
      "   ----- ---------------------------------- 37.3/284.2 MB 8.4 MB/s eta 0:00:30\n",
      "   ----- ---------------------------------- 38.1/284.2 MB 8.7 MB/s eta 0:00:29\n",
      "   ----- ---------------------------------- 39.9/284.2 MB 10.1 MB/s eta 0:00:25\n",
      "   ----- ---------------------------------- 42.1/284.2 MB 12.6 MB/s eta 0:00:20\n",
      "   ------ --------------------------------- 44.1/284.2 MB 15.2 MB/s eta 0:00:16\n",
      "   ------ --------------------------------- 45.4/284.2 MB 18.2 MB/s eta 0:00:14\n",
      "   ------ --------------------------------- 48.1/284.2 MB 40.9 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 50.3/284.2 MB 46.7 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 52.4/284.2 MB 46.9 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 54.3/284.2 MB 46.9 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 56.2/284.2 MB 46.7 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 58.1/284.2 MB 43.7 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 60.2/284.2 MB 43.7 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 62.4/284.2 MB 43.5 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 64.5/284.2 MB 43.5 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 66.3/284.2 MB 43.5 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 67.8/284.2 MB 40.9 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 69.4/284.2 MB 40.9 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 70.8/284.2 MB 38.6 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 71.9/284.2 MB 36.4 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 73.8/284.2 MB 34.4 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 75.6/284.2 MB 34.4 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 77.1/284.2 MB 32.7 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 78.4/284.2 MB 34.4 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 80.3/284.2 MB 34.6 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 81.8/284.2 MB 32.8 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 83.3/284.2 MB 34.4 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 84.8/284.2 MB 32.8 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 86.5/284.2 MB 32.7 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 88.1/284.2 MB 34.4 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 89.6/284.2 MB 34.4 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 91.1/284.2 MB 34.4 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 92.7/284.2 MB 34.6 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 94.3/284.2 MB 32.8 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 95.7/284.2 MB 32.8 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 97.4/284.2 MB 32.8 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 99.0/284.2 MB 32.7 MB/s eta 0:00:06\n",
      "   ------------- ------------------------- 100.6/284.2 MB 34.4 MB/s eta 0:00:06\n",
      "   -------------- ------------------------ 102.1/284.2 MB 32.7 MB/s eta 0:00:06\n",
      "   -------------- ------------------------ 103.5/284.2 MB 34.4 MB/s eta 0:00:06\n",
      "   -------------- ------------------------ 105.2/284.2 MB 34.4 MB/s eta 0:00:06\n",
      "   -------------- ------------------------ 106.6/284.2 MB 34.4 MB/s eta 0:00:06\n",
      "   -------------- ------------------------ 108.2/284.2 MB 34.4 MB/s eta 0:00:06\n",
      "   --------------- ----------------------- 109.8/284.2 MB 34.6 MB/s eta 0:00:06\n",
      "   --------------- ----------------------- 111.4/284.2 MB 34.4 MB/s eta 0:00:06\n",
      "   --------------- ----------------------- 112.9/284.2 MB 34.4 MB/s eta 0:00:05\n",
      "   --------------- ----------------------- 114.4/284.2 MB 34.4 MB/s eta 0:00:05\n",
      "   --------------- ----------------------- 116.0/284.2 MB 34.4 MB/s eta 0:00:05\n",
      "   ---------------- ---------------------- 117.6/284.2 MB 34.4 MB/s eta 0:00:05\n",
      "   ---------------- ---------------------- 119.1/284.2 MB 34.4 MB/s eta 0:00:05\n",
      "   ---------------- ---------------------- 120.7/284.2 MB 34.4 MB/s eta 0:00:05\n",
      "   ---------------- ---------------------- 122.2/284.2 MB 34.6 MB/s eta 0:00:05\n",
      "   ---------------- ---------------------- 123.8/284.2 MB 34.4 MB/s eta 0:00:05\n",
      "   ----------------- --------------------- 125.5/284.2 MB 34.4 MB/s eta 0:00:05\n",
      "   ----------------- --------------------- 126.8/284.2 MB 34.4 MB/s eta 0:00:05\n",
      "   ----------------- --------------------- 128.5/284.2 MB 34.4 MB/s eta 0:00:05\n",
      "   ----------------- --------------------- 130.0/284.2 MB 34.4 MB/s eta 0:00:05\n",
      "   ------------------ -------------------- 131.6/284.2 MB 34.4 MB/s eta 0:00:05\n",
      "   ------------------ -------------------- 133.1/284.2 MB 34.4 MB/s eta 0:00:05\n",
      "   ------------------ -------------------- 134.7/284.2 MB 34.6 MB/s eta 0:00:05\n",
      "   ------------------ -------------------- 136.1/284.2 MB 32.8 MB/s eta 0:00:05\n",
      "   ------------------ -------------------- 137.7/284.2 MB 32.8 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 139.3/284.2 MB 32.8 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 140.8/284.2 MB 32.7 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 142.4/284.2 MB 32.7 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 143.9/284.2 MB 32.7 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 145.5/284.2 MB 32.7 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 147.0/284.2 MB 32.8 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 148.3/284.2 MB 34.4 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 150.1/284.2 MB 32.8 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 151.7/284.2 MB 32.8 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 153.3/284.2 MB 32.7 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 154.8/284.2 MB 32.7 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 156.3/284.2 MB 34.4 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 157.9/284.2 MB 34.4 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 158.9/284.2 MB 31.2 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 159.3/284.2 MB 29.7 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 159.7/284.2 MB 27.3 MB/s eta 0:00:05\n",
      "   --------------------- ----------------- 160.2/284.2 MB 25.2 MB/s eta 0:00:05\n",
      "   ---------------------- ---------------- 160.5/284.2 MB 22.6 MB/s eta 0:00:06\n",
      "   ---------------------- ---------------- 161.0/284.2 MB 21.1 MB/s eta 0:00:06\n",
      "   ---------------------- ---------------- 161.4/284.2 MB 19.8 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 161.7/284.2 MB 18.2 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 162.1/284.2 MB 17.2 MB/s eta 0:00:08\n",
      "   ---------------------- ---------------- 162.5/284.2 MB 16.4 MB/s eta 0:00:08\n",
      "   ---------------------- ---------------- 162.7/284.2 MB 15.6 MB/s eta 0:00:08\n",
      "   ---------------------- ---------------- 162.7/284.2 MB 15.6 MB/s eta 0:00:08\n",
      "   ---------------------- ---------------- 163.4/284.2 MB 13.6 MB/s eta 0:00:09\n",
      "   ---------------------- ---------------- 165.3/284.2 MB 13.9 MB/s eta 0:00:09\n",
      "   ---------------------- ---------------- 167.4/284.2 MB 14.2 MB/s eta 0:00:09\n",
      "   ----------------------- --------------- 169.5/284.2 MB 15.6 MB/s eta 0:00:08\n",
      "   ----------------------- --------------- 171.6/284.2 MB 21.8 MB/s eta 0:00:06\n",
      "   ----------------------- --------------- 173.6/284.2 MB 43.5 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 175.7/284.2 MB 46.7 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 177.7/284.2 MB 43.7 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 179.9/284.2 MB 43.7 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 181.9/284.2 MB 43.7 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 183.9/284.2 MB 43.7 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 185.8/284.2 MB 46.9 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 188.1/284.2 MB 43.5 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 190.0/284.2 MB 43.5 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 191.4/284.2 MB 43.7 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 193.0/284.2 MB 40.9 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 194.6/284.2 MB 40.9 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 196.1/284.2 MB 38.5 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 197.8/284.2 MB 36.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 199.3/284.2 MB 34.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 200.9/284.2 MB 34.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 202.3/284.2 MB 34.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 203.7/284.2 MB 32.7 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 205.5/284.2 MB 34.4 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 207.0/284.2 MB 34.4 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 208.7/284.2 MB 34.4 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 210.2/284.2 MB 34.6 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 211.8/284.2 MB 34.4 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 213.1/284.2 MB 34.4 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 214.8/284.2 MB 34.4 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 216.4/284.2 MB 34.4 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 217.9/284.2 MB 32.7 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 219.5/284.2 MB 34.4 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 221.0/284.2 MB 32.7 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 222.6/284.2 MB 34.6 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 224.1/284.2 MB 34.4 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 225.8/284.2 MB 32.8 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 227.3/284.2 MB 34.4 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 228.8/284.2 MB 32.7 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 230.2/284.2 MB 32.7 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 231.8/284.2 MB 32.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 233.5/284.2 MB 32.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 234.9/284.2 MB 32.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 236.5/284.2 MB 32.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 238.1/284.2 MB 32.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 239.6/284.2 MB 32.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 240.7/284.2 MB 31.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 241.3/284.2 MB 29.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 241.9/284.2 MB 27.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 242.5/284.2 MB 26.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 243.1/284.2 MB 24.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 243.6/284.2 MB 22.6 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 244.1/284.2 MB 21.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 244.6/284.2 MB 19.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 244.9/284.2 MB 18.2 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 245.1/284.2 MB 17.2 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 245.7/284.2 MB 16.4 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 246.1/284.2 MB 15.6 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 246.5/284.2 MB 14.6 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 246.8/284.2 MB 13.9 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 247.2/284.2 MB 13.4 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 247.5/284.2 MB 12.6 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 247.9/284.2 MB 12.4 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 248.1/284.2 MB 11.5 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 248.5/284.2 MB 11.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 248.8/284.2 MB 10.9 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 249.3/284.2 MB 10.4 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 249.8/284.2 MB 10.2 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 249.9/284.2 MB 10.1 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 250.4/284.2 MB 9.5 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 251.9/284.2 MB 9.9 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 253.7/284.2 MB 10.9 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 254.3/284.2 MB 11.1 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 254.8/284.2 MB 11.1 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 255.6/284.2 MB 12.1 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 257.5/284.2 MB 15.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 259.1/284.2 MB 19.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 259.8/284.2 MB 21.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 260.8/284.2 MB 25.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 262.8/284.2 MB 25.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 263.9/284.2 MB 25.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 265.3/284.2 MB 29.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 267.4/284.2 MB 31.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 269.1/284.2 MB 29.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 271.2/284.2 MB 36.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 272.5/284.2 MB 36.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 272.6/284.2 MB 29.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 273.7/284.2 MB 29.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 276.1/284.2 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  278.1/284.2 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  280.0/284.2 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  282.0/284.2 MB 32.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  284.2/284.2 MB 43.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  284.2/284.2 MB 43.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  284.2/284.2 MB 43.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  284.2/284.2 MB 43.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  284.2/284.2 MB 43.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  284.2/284.2 MB 43.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  284.2/284.2 MB 43.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  284.2/284.2 MB 43.7 MB/s eta 0:00:01\n",
      "   --------------------------------------- 284.2/284.2 MB 12.4 MB/s eta 0:00:00\n",
      "Downloading ml_dtypes-0.2.0-cp311-cp311-win_amd64.whl (938 kB)\n",
      "   ---------------------------------------- 0.0/938.7 kB ? eta -:--:--\n",
      "   --------------------------------------  931.8/938.7 kB 19.6 MB/s eta 0:00:01\n",
      "   --------------------------------------- 938.7/938.7 kB 19.8 MB/s eta 0:00:00\n",
      "Downloading absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "   ---------------------------------------- 0.0/130.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 130.2/130.2 kB ? eta 0:00:00\n",
      "Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Downloading grpcio-1.59.0-cp311-cp311-win_amd64.whl (3.7 MB)\n",
      "   ---------------------------------------- 0.0/3.7 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 1.5/3.7 MB 48.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 3.2/3.7 MB 41.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.7/3.7 MB 38.8 MB/s eta 0:00:00\n",
      "Downloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------  1.7/1.7 MB 54.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 36.1 MB/s eta 0:00:00\n",
      "Downloading libclang-16.0.6-py2.py3-none-win_amd64.whl (24.4 MB)\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.1/24.4 MB 36.7 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 3.1/24.4 MB 39.8 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 4.1/24.4 MB 32.4 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 4.4/24.4 MB 25.7 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 4.7/24.4 MB 21.5 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 5.0/24.4 MB 18.8 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 5.3/24.4 MB 16.9 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 5.5/24.4 MB 15.4 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 5.7/24.4 MB 14.1 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 6.0/24.4 MB 13.3 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 6.2/24.4 MB 12.9 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 6.6/24.4 MB 12.1 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 7.1/24.4 MB 11.9 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 7.4/24.4 MB 11.5 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 7.6/24.4 MB 11.4 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 8.0/24.4 MB 11.2 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 8.4/24.4 MB 11.0 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 8.8/24.4 MB 10.9 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 9.3/24.4 MB 10.8 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 9.8/24.4 MB 10.8 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 10.3/24.4 MB 10.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 10.7/24.4 MB 10.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 11.0/24.4 MB 10.1 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 11.6/24.4 MB 9.8 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 12.1/24.4 MB 9.5 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 12.5/24.4 MB 9.1 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 12.8/24.4 MB 8.8 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 13.2/24.4 MB 8.5 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 13.8/24.4 MB 8.3 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 14.3/24.4 MB 8.2 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 14.6/24.4 MB 8.2 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 14.9/24.4 MB 8.3 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 15.3/24.4 MB 8.4 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 15.5/24.4 MB 8.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 16.1/24.4 MB 8.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 18.2/24.4 MB 10.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 20.2/24.4 MB 13.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 21.6/24.4 MB 15.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.2/24.4 MB 16.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.4 MB 21.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.4 MB 21.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.4/24.4 MB 19.8 MB/s eta 0:00:00\n",
      "Downloading protobuf-4.24.4-cp310-abi3-win_amd64.whl (430 kB)\n",
      "   ---------------------------------------- 0.0/430.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 430.5/430.5 kB 26.3 MB/s eta 0:00:00\n",
      "Downloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 2.1/5.5 MB 67.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 3.7/5.5 MB 47.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.5/5.5 MB 43.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 39.0 MB/s eta 0:00:00\n",
      "Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
      "   ---------------------------------------- 0.0/440.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 440.7/440.7 kB 26.9 MB/s eta 0:00:00\n",
      "Downloading google_auth-2.23.3-py2.py3-none-any.whl (182 kB)\n",
      "   ---------------------------------------- 0.0/182.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 182.3/182.3 kB 11.5 MB/s eta 0:00:00\n",
      "Downloading tensorboard_data_server-0.7.1-py3-none-any.whl (2.4 kB)\n",
      "Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Installing collected packages: libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, protobuf, opt-einsum, oauthlib, ml-dtypes, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-2.0.0 astunparse-1.6.3 cachetools-5.3.1 flatbuffers-23.5.26 gast-0.5.4 google-auth-2.23.3 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.59.0 keras-2.14.0 libclang-16.0.6 ml-dtypes-0.2.0 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.24.4 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.14.1 tensorboard-data-server-0.7.1 tensorflow-2.14.0 tensorflow-estimator-2.14.0 tensorflow-intel-2.14.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.3.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9061ff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d8c2cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path1 = 'C:/Users/poorn/CyberSecData/'\n",
    "file_path1 = project_path1 + 'GL_Bot_New.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b830aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the json corpus data\n",
    "import json\n",
    "json_data = json.loads( open( file_path1 ).read( ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "892bc23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intents': [{'tag': 'Intro', 'patterns': ['hi', 'how are you', 'is anyone there', 'hello', 'whats up', 'hey', 'yo', 'listen', 'please help me', 'i am learner from', 'i belong to', 'aiml batch', 'aifl batch', 'i am from', 'my pm is', 'blended', 'online', 'i am from', 'hey ya', 'talking to you for first time'], 'responses': ['Hello! how can i help you ?'], 'context_set': ''}, {'tag': 'Exit', 'patterns': ['thank you', 'thanks', 'cya', 'see you', 'later', 'see you later', 'goodbye', 'i am leaving', 'have a Good day', 'you helped me', 'thanks a lot', 'thanks a ton', 'you are the best', 'great help', 'too good', 'you are a good learning buddy'], 'responses': ['I hope I was able to assist you, Good Bye'], 'context_set': ''}, {'tag': 'Olympus', 'patterns': ['olympus', 'explain me how olympus works', 'I am not able to understand olympus', 'olympus window not working', 'no access to olympus', 'unable to see link in olympus', 'no link visible on olympus', 'whom to contact for olympus', 'lot of problem with olympus', 'olypus is not a good tool', 'lot of problems with olympus', 'how to use olympus', 'teach me olympus'], 'responses': ['Link: Olympus wiki'], 'context_set': ''}, {'tag': 'SL', 'patterns': ['i am not able to understand svm', 'explain me how machine learning works', 'i am not able to understand naive bayes', 'i am not able to understand logistic regression', 'i am not able to understand ensemble techniques', 'i am not able to understand knn', 'i am not able to understand knn imputer', 'i am not able to understand cross validation', 'i am not able to understand boosting', 'i am not able to understand random forest', 'i am not able to understand ada boosting', 'i am not able to understand gradient boosting', 'machine learning', 'ML', 'SL', 'supervised learning', 'knn', 'logistic regression', 'regression', 'classification', 'naive bayes', 'nb', 'ensemble techniques', 'bagging', 'boosting', 'ada boosting', 'ada', 'gradient boosting', 'hyper parameters'], 'responses': ['Link: Machine Learning wiki '], 'context_set': ''}, {'tag': 'NN', 'patterns': ['what is deep learning', 'unable to understand deep learning', 'explain me how deep learning works', 'i am not able to understand deep learning', 'not able to understand neural nets', 'very diffult to understand neural nets', 'unable to understand neural nets', 'ann', 'artificial intelligence', 'artificial neural networks', 'weights', 'activation function', 'hidden layers', 'softmax', 'sigmoid', 'relu', 'otimizer', 'forward propagation', 'backward propagation', 'epochs', 'epoch', 'what is an epoch', 'adam', 'sgd'], 'responses': ['Link: Neural Nets wiki'], 'context_set': ''}, {'tag': 'Bot', 'patterns': ['what is your name', 'who are you', 'name please', 'when are your hours of opertions', 'what are your working hours', 'hours of operation', 'working hours', 'hours'], 'responses': ['I am your virtual learning assistant'], 'context_set': ''}, {'tag': 'Profane', 'patterns': ['what the hell', 'bloody stupid bot', 'do you think you are very smart', 'screw you', 'i hate you', 'you are stupid', 'jerk', 'you are a joke', 'useless piece of shit'], 'responses': ['Please use respectful words'], 'context_set': ''}, {'tag': 'Ticket', 'patterns': ['my problem is not solved', 'you did not help me', 'not a good solution', 'bad solution', 'not good solution', 'no help', 'wasted my time', 'useless bot', 'create a ticket'], 'responses': ['Transferring the request to your PM'], 'context_set': ''}, {'tag': 'GL', 'patterns': ['Great Learning', 'Certificate Courses', 'AIML', 'Data Science and Business Analytics', 'Management', 'Bootcamps', 'Cloud Computing', 'Cyber Security', 'Software Development', 'Digital Marketing & Sales', 'Design Thinking'], 'responses': ['Please visit www.MyGreatLearning.com', 'Explore the Great Learning', 'Learn new skills with Great Learning Program'], 'context_set': ''}]}\n"
     ]
    }
   ],
   "source": [
    "print(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a464d608",
   "metadata": {},
   "outputs": [],
   "source": [
    "corps_voc = set()\n",
    "corps_tags = []\n",
    "corps_word_tag = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a55fee5",
   "metadata": {},
   "source": [
    "{\"intents\": [\n",
    "        {\"tag\": \"Intro\",\n",
    "         \"patterns\": [\"hi\", \n",
    "                      \"how are you\", \n",
    "                      \"online\",\n",
    "                      \"i am from\",\n",
    "                      \"hey ya\",\n",
    "                      \"talking to you for first time\"],\n",
    "         \"responses\": [\"Hello! how can i help you ?\"],\n",
    "         \"context_set\": \"\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1a6549f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate JSOn data and load contents (Corpus has Intents that has a structure\n",
    "# tag - High level group\n",
    "# patterns - possible inputs from user for training the model\n",
    "# response -  List of responses that can be provided randomly if more than one response is available\n",
    "# \n",
    "for intent_itm in json_data['intents']:\n",
    "    tag = intent_itm['tag']\n",
    "    corps_tags.append( tag ) #  add tag value\n",
    "    for pattern in intent_itm['patterns']:\n",
    "        words = word_tokenize( pattern )   #  Break the pattern row words\n",
    "        corps_voc.update( words )          #  collect unique word\n",
    "        corps_word_tag.append( ( words, tag ) )   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a7e73c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corps_voc = list( corps_voc )  # change from set to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ed1cf621",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "53a26c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\poorn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\poorn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use WordNet alongside the NLTK module to find the meanings of words, synonyms, antonyms, and more. \n",
    "nltk.download('wordnet') \n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "68641d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of spl characters\n",
    "spl_chars = ['~', ':', \"'\", '+', '[', '\\\\', '@', '^', '{', '%', '(', '-', '\"', '*', '|', ',', '&', '<', '`', '}', '.', '_', '=', ']', '!', '>', ';', '?', '#', '$', ')', '/']\n",
    "wnl = WordNetLemmatizer()\n",
    "corps_voc = [ wnl.lemmatize(word)  for word in corps_voc if word not in spl_chars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7e32db6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5c33c82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = []\n",
    "output_empty = [0] * len(corps_tags)\n",
    "output_empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9d3c5d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139\n",
      "184\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print(len(corps_word_tag)) # No of sentences in [patterns]\n",
    "print(len(corps_voc))  #  # of words across all sentences in [patterns]\n",
    "print(len(corps_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5de7f222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi'] Intro\n",
      "['how', 'are', 'you'] Intro\n",
      "['is', 'anyone', 'there'] Intro\n",
      "['hello'] Intro\n",
      "['whats', 'up'] Intro\n",
      "['hey'] Intro\n",
      "['yo'] Intro\n",
      "['listen'] Intro\n",
      "['please', 'help', 'me'] Intro\n",
      "['i', 'am', 'learner', 'from'] Intro\n",
      "['i', 'belong', 'to'] Intro\n",
      "['aiml', 'batch'] Intro\n",
      "['aifl', 'batch'] Intro\n",
      "['i', 'am', 'from'] Intro\n",
      "['my', 'pm', 'is'] Intro\n",
      "['blended'] Intro\n",
      "['online'] Intro\n",
      "['i', 'am', 'from'] Intro\n",
      "['hey', 'ya'] Intro\n",
      "['talking', 'to', 'you', 'for', 'first', 'time'] Intro\n",
      "['thank', 'you'] Exit\n",
      "['thanks'] Exit\n",
      "['cya'] Exit\n",
      "['see', 'you'] Exit\n",
      "['later'] Exit\n",
      "['see', 'you', 'later'] Exit\n",
      "['goodbye'] Exit\n",
      "['i', 'am', 'leaving'] Exit\n",
      "['have', 'a', 'Good', 'day'] Exit\n",
      "['you', 'helped', 'me'] Exit\n",
      "['thanks', 'a', 'lot'] Exit\n",
      "['thanks', 'a', 'ton'] Exit\n",
      "['you', 'are', 'the', 'best'] Exit\n",
      "['great', 'help'] Exit\n",
      "['too', 'good'] Exit\n",
      "['you', 'are', 'a', 'good', 'learning', 'buddy'] Exit\n",
      "['olympus'] Olympus\n",
      "['explain', 'me', 'how', 'olympus', 'works'] Olympus\n",
      "['I', 'am', 'not', 'able', 'to', 'understand', 'olympus'] Olympus\n",
      "['olympus', 'window', 'not', 'working'] Olympus\n",
      "['no', 'access', 'to', 'olympus'] Olympus\n",
      "['unable', 'to', 'see', 'link', 'in', 'olympus'] Olympus\n",
      "['no', 'link', 'visible', 'on', 'olympus'] Olympus\n",
      "['whom', 'to', 'contact', 'for', 'olympus'] Olympus\n",
      "['lot', 'of', 'problem', 'with', 'olympus'] Olympus\n",
      "['olypus', 'is', 'not', 'a', 'good', 'tool'] Olympus\n",
      "['lot', 'of', 'problems', 'with', 'olympus'] Olympus\n",
      "['how', 'to', 'use', 'olympus'] Olympus\n",
      "['teach', 'me', 'olympus'] Olympus\n",
      "['i', 'am', 'not', 'able', 'to', 'understand', 'svm'] SL\n",
      "['explain', 'me', 'how', 'machine', 'learning', 'works'] SL\n",
      "['i', 'am', 'not', 'able', 'to', 'understand', 'naive', 'bayes'] SL\n",
      "['i', 'am', 'not', 'able', 'to', 'understand', 'logistic', 'regression'] SL\n",
      "['i', 'am', 'not', 'able', 'to', 'understand', 'ensemble', 'techniques'] SL\n",
      "['i', 'am', 'not', 'able', 'to', 'understand', 'knn'] SL\n",
      "['i', 'am', 'not', 'able', 'to', 'understand', 'knn', 'imputer'] SL\n",
      "['i', 'am', 'not', 'able', 'to', 'understand', 'cross', 'validation'] SL\n",
      "['i', 'am', 'not', 'able', 'to', 'understand', 'boosting'] SL\n",
      "['i', 'am', 'not', 'able', 'to', 'understand', 'random', 'forest'] SL\n",
      "['i', 'am', 'not', 'able', 'to', 'understand', 'ada', 'boosting'] SL\n",
      "['i', 'am', 'not', 'able', 'to', 'understand', 'gradient', 'boosting'] SL\n",
      "['machine', 'learning'] SL\n",
      "['ML'] SL\n",
      "['SL'] SL\n",
      "['supervised', 'learning'] SL\n",
      "['knn'] SL\n",
      "['logistic', 'regression'] SL\n",
      "['regression'] SL\n",
      "['classification'] SL\n",
      "['naive', 'bayes'] SL\n",
      "['nb'] SL\n",
      "['ensemble', 'techniques'] SL\n",
      "['bagging'] SL\n",
      "['boosting'] SL\n",
      "['ada', 'boosting'] SL\n",
      "['ada'] SL\n",
      "['gradient', 'boosting'] SL\n",
      "['hyper', 'parameters'] SL\n",
      "['what', 'is', 'deep', 'learning'] NN\n",
      "['unable', 'to', 'understand', 'deep', 'learning'] NN\n",
      "['explain', 'me', 'how', 'deep', 'learning', 'works'] NN\n",
      "['i', 'am', 'not', 'able', 'to', 'understand', 'deep', 'learning'] NN\n",
      "['not', 'able', 'to', 'understand', 'neural', 'nets'] NN\n",
      "['very', 'diffult', 'to', 'understand', 'neural', 'nets'] NN\n",
      "['unable', 'to', 'understand', 'neural', 'nets'] NN\n",
      "['ann'] NN\n",
      "['artificial', 'intelligence'] NN\n",
      "['artificial', 'neural', 'networks'] NN\n",
      "['weights'] NN\n",
      "['activation', 'function'] NN\n",
      "['hidden', 'layers'] NN\n",
      "['softmax'] NN\n",
      "['sigmoid'] NN\n",
      "['relu'] NN\n",
      "['otimizer'] NN\n",
      "['forward', 'propagation'] NN\n",
      "['backward', 'propagation'] NN\n",
      "['epochs'] NN\n",
      "['epoch'] NN\n",
      "['what', 'is', 'an', 'epoch'] NN\n",
      "['adam'] NN\n",
      "['sgd'] NN\n",
      "['what', 'is', 'your', 'name'] Bot\n",
      "['who', 'are', 'you'] Bot\n",
      "['name', 'please'] Bot\n",
      "['when', 'are', 'your', 'hours', 'of', 'opertions'] Bot\n",
      "['what', 'are', 'your', 'working', 'hours'] Bot\n",
      "['hours', 'of', 'operation'] Bot\n",
      "['working', 'hours'] Bot\n",
      "['hours'] Bot\n",
      "['what', 'the', 'hell'] Profane\n",
      "['bloody', 'stupid', 'bot'] Profane\n",
      "['do', 'you', 'think', 'you', 'are', 'very', 'smart'] Profane\n",
      "['screw', 'you'] Profane\n",
      "['i', 'hate', 'you'] Profane\n",
      "['you', 'are', 'stupid'] Profane\n",
      "['jerk'] Profane\n",
      "['you', 'are', 'a', 'joke'] Profane\n",
      "['useless', 'piece', 'of', 'shit'] Profane\n",
      "['my', 'problem', 'is', 'not', 'solved'] Ticket\n",
      "['you', 'did', 'not', 'help', 'me'] Ticket\n",
      "['not', 'a', 'good', 'solution'] Ticket\n",
      "['bad', 'solution'] Ticket\n",
      "['not', 'good', 'solution'] Ticket\n",
      "['no', 'help'] Ticket\n",
      "['wasted', 'my', 'time'] Ticket\n",
      "['useless', 'bot'] Ticket\n",
      "['create', 'a', 'ticket'] Ticket\n",
      "['Great', 'Learning'] GL\n",
      "['Certificate', 'Courses'] GL\n",
      "['AIML'] GL\n",
      "['Data', 'Science', 'and', 'Business', 'Analytics'] GL\n",
      "['Management'] GL\n",
      "['Bootcamps'] GL\n",
      "['Cloud', 'Computing'] GL\n",
      "['Cyber', 'Security'] GL\n",
      "['Software', 'Development'] GL\n",
      "['Digital', 'Marketing', '&', 'Sales'] GL\n",
      "['Design', 'Thinking'] GL\n"
     ]
    }
   ],
   "source": [
    "for docu in corps_word_tag:\n",
    "    print(docu[0], docu[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "04bc57be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "for document in corps_word_tag:\n",
    "    bag = []\n",
    "    word_patterns = document[0]\n",
    "    #word_patterns = [ wnl.lemmatize (word.lower())  for word in word_patterns]\n",
    "    word_patterns = [ wnl.lemmatize ( x.lower() )  for x in word_patterns ]\n",
    "    for word in corps_voc:\n",
    "        bag.append(1) if word in word_patterns else bag.append(0)\n",
    "    output_row = list(output_empty)\n",
    "\n",
    "    output_row[ corps_tags.index(document[1])] = 1\n",
    "    print(output_row)\n",
    "    training_data.append( [ bag,  output_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2ab63913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data \n",
    "training_data = []\n",
    "output_empty = [0] * len(corps_tags)\n",
    "for document in corps_word_tag:\n",
    "  bag = []\n",
    "  word_patterns = document[0]\n",
    "  word_patterns = [wnl.lemmatize (word.lower()) for word in word_patterns]\n",
    "  for word in corps_voc:\n",
    "    bag.append(1) if word in word_patterns else bag. append (0)\n",
    "  output_row = list (output_empty)\n",
    "  output_row[corps_tags.index(document[1])] = 1\n",
    "  training_data.append ([bag, output_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "33882141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n",
      "184 9\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data))\n",
    "for td in training_data:\n",
    "    print(len(td[0]),  len(td[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0eaad93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random.shuffle(training_data)\n",
    "Data_type = str\n",
    "training_data = np.array(training_data, dtype=object)\n",
    "X_train = list(training_data[:, 0])\n",
    "y_train = list(training_data[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1d57fadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(139, 2)\n",
      "184\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print(training_data.shape)\n",
    "print(len(X_train[0]))\n",
    "print(len(y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7e82813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add (Dense (128, input_shape=(len(X_train[0]),), activation='relu'))\n",
    "model.add (Dropout(0.5))\n",
    "model.add( Dense( 64, activation='relu' ) )\n",
    "model.add (Dropout(0.5))\n",
    "model.add( Dense( 32, activation='relu' ) )\n",
    "\n",
    "model.add( Dense( len(y_train[0]), activation='softmax') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a1c7a737",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_sgd = SGD(learning_rate=0.01)\n",
    "model.compile( loss='categorical_crossentropy', optimizer=opt_sgd , metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3c3ecc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 2.2223 - accuracy: 0.0863\n",
      "Epoch 2/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 2.2023 - accuracy: 0.1367\n",
      "Epoch 3/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 2.1801 - accuracy: 0.1511\n",
      "Epoch 4/200\n",
      "28/28 [==============================] - 0s 945us/step - loss: 2.1770 - accuracy: 0.1583\n",
      "Epoch 5/200\n",
      "28/28 [==============================] - 0s 963us/step - loss: 2.1620 - accuracy: 0.1439\n",
      "Epoch 6/200\n",
      "28/28 [==============================] - 0s 963us/step - loss: 2.1503 - accuracy: 0.2374\n",
      "Epoch 7/200\n",
      "28/28 [==============================] - 0s 908us/step - loss: 2.1379 - accuracy: 0.2302\n",
      "Epoch 8/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 2.1336 - accuracy: 0.2014\n",
      "Epoch 9/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 2.1197 - accuracy: 0.2158\n",
      "Epoch 10/200\n",
      "28/28 [==============================] - 0s 968us/step - loss: 2.0846 - accuracy: 0.2230\n",
      "Epoch 11/200\n",
      "28/28 [==============================] - 0s 908us/step - loss: 2.0923 - accuracy: 0.2590\n",
      "Epoch 12/200\n",
      "28/28 [==============================] - 0s 927us/step - loss: 2.0817 - accuracy: 0.2302\n",
      "Epoch 13/200\n",
      "28/28 [==============================] - 0s 963us/step - loss: 2.0783 - accuracy: 0.2374\n",
      "Epoch 14/200\n",
      "28/28 [==============================] - 0s 901us/step - loss: 2.0731 - accuracy: 0.2590\n",
      "Epoch 15/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 2.0590 - accuracy: 0.2446\n",
      "Epoch 16/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 2.0561 - accuracy: 0.2878\n",
      "Epoch 17/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 2.0435 - accuracy: 0.2446\n",
      "Epoch 18/200\n",
      "28/28 [==============================] - 0s 945us/step - loss: 2.0536 - accuracy: 0.2518\n",
      "Epoch 19/200\n",
      "28/28 [==============================] - 0s 951us/step - loss: 2.0354 - accuracy: 0.2734\n",
      "Epoch 20/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 2.0466 - accuracy: 0.2230\n",
      "Epoch 21/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 2.0283 - accuracy: 0.2662\n",
      "Epoch 22/200\n",
      "28/28 [==============================] - 0s 908us/step - loss: 2.0157 - accuracy: 0.2302\n",
      "Epoch 23/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 1.9995 - accuracy: 0.2734\n",
      "Epoch 24/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 1.9976 - accuracy: 0.2158\n",
      "Epoch 25/200\n",
      "28/28 [==============================] - 0s 908us/step - loss: 2.0087 - accuracy: 0.2734\n",
      "Epoch 26/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 1.9976 - accuracy: 0.3022\n",
      "Epoch 27/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 1.9709 - accuracy: 0.2374\n",
      "Epoch 28/200\n",
      "28/28 [==============================] - 0s 963us/step - loss: 1.9758 - accuracy: 0.2518\n",
      "Epoch 29/200\n",
      "28/28 [==============================] - 0s 908us/step - loss: 1.9626 - accuracy: 0.2806\n",
      "Epoch 30/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 1.9631 - accuracy: 0.2590\n",
      "Epoch 31/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 1.9452 - accuracy: 0.3094\n",
      "Epoch 32/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 1.9518 - accuracy: 0.2662\n",
      "Epoch 33/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 1.9317 - accuracy: 0.2806\n",
      "Epoch 34/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 1.9300 - accuracy: 0.2590\n",
      "Epoch 35/200\n",
      "28/28 [==============================] - 0s 963us/step - loss: 1.9015 - accuracy: 0.3094\n",
      "Epoch 36/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.9101 - accuracy: 0.3022\n",
      "Epoch 37/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.9009 - accuracy: 0.3381\n",
      "Epoch 38/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.8753 - accuracy: 0.3525\n",
      "Epoch 39/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.8670 - accuracy: 0.2950\n",
      "Epoch 40/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.8450 - accuracy: 0.3165\n",
      "Epoch 41/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.8709 - accuracy: 0.2878\n",
      "Epoch 42/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.8446 - accuracy: 0.3309\n",
      "Epoch 43/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.8262 - accuracy: 0.3453\n",
      "Epoch 44/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.7736 - accuracy: 0.3381\n",
      "Epoch 45/200\n",
      "28/28 [==============================] - 0s 982us/step - loss: 1.7789 - accuracy: 0.4101\n",
      "Epoch 46/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.8260 - accuracy: 0.3094\n",
      "Epoch 47/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.7880 - accuracy: 0.3957\n",
      "Epoch 48/200\n",
      "28/28 [==============================] - 0s 929us/step - loss: 1.7081 - accuracy: 0.4029\n",
      "Epoch 49/200\n",
      "28/28 [==============================] - 0s 963us/step - loss: 1.7355 - accuracy: 0.4388\n",
      "Epoch 50/200\n",
      "28/28 [==============================] - 0s 937us/step - loss: 1.7380 - accuracy: 0.4101\n",
      "Epoch 51/200\n",
      "28/28 [==============================] - 0s 945us/step - loss: 1.7061 - accuracy: 0.4173\n",
      "Epoch 52/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 1.6808 - accuracy: 0.4388\n",
      "Epoch 53/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 1.7189 - accuracy: 0.3741\n",
      "Epoch 54/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 1.5993 - accuracy: 0.4676\n",
      "Epoch 55/200\n",
      "28/28 [==============================] - 0s 945us/step - loss: 1.5773 - accuracy: 0.4820\n",
      "Epoch 56/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 1.6242 - accuracy: 0.4460\n",
      "Epoch 57/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 1.5589 - accuracy: 0.4676\n",
      "Epoch 58/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 1.5635 - accuracy: 0.4964\n",
      "Epoch 59/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 1.6109 - accuracy: 0.4820\n",
      "Epoch 60/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 1.5484 - accuracy: 0.4676\n",
      "Epoch 61/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 1.5527 - accuracy: 0.4388\n",
      "Epoch 62/200\n",
      "28/28 [==============================] - 0s 908us/step - loss: 1.5082 - accuracy: 0.5108\n",
      "Epoch 63/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 1.4749 - accuracy: 0.4748\n",
      "Epoch 64/200\n",
      "28/28 [==============================] - 0s 963us/step - loss: 1.5305 - accuracy: 0.4101\n",
      "Epoch 65/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 1.4549 - accuracy: 0.5396\n",
      "Epoch 66/200\n",
      "28/28 [==============================] - 0s 871us/step - loss: 1.4385 - accuracy: 0.4892\n",
      "Epoch 67/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 1.4035 - accuracy: 0.5324\n",
      "Epoch 68/200\n",
      "28/28 [==============================] - 0s 901us/step - loss: 1.4855 - accuracy: 0.5468\n",
      "Epoch 69/200\n",
      "28/28 [==============================] - 0s 906us/step - loss: 1.4392 - accuracy: 0.5108\n",
      "Epoch 70/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 1.3930 - accuracy: 0.5324\n",
      "Epoch 71/200\n",
      "28/28 [==============================] - 0s 1000us/step - loss: 1.3408 - accuracy: 0.5683\n",
      "Epoch 72/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3190 - accuracy: 0.5755\n",
      "Epoch 73/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.2809 - accuracy: 0.5683\n",
      "Epoch 74/200\n",
      "28/28 [==============================] - 0s 963us/step - loss: 1.2742 - accuracy: 0.5612\n",
      "Epoch 75/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3293 - accuracy: 0.5252\n",
      "Epoch 76/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.2022 - accuracy: 0.5971\n",
      "Epoch 77/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.2368 - accuracy: 0.5827\n",
      "Epoch 78/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.2464 - accuracy: 0.5612\n",
      "Epoch 79/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 1.2209 - accuracy: 0.5827\n",
      "Epoch 80/200\n",
      "28/28 [==============================] - 0s 963us/step - loss: 1.2017 - accuracy: 0.6115\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 926us/step - loss: 1.1879 - accuracy: 0.6043\n",
      "Epoch 82/200\n",
      "28/28 [==============================] - 0s 963us/step - loss: 1.1752 - accuracy: 0.5899\n",
      "Epoch 83/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.1226 - accuracy: 0.6331\n",
      "Epoch 84/200\n",
      "28/28 [==============================] - 0s 963us/step - loss: 1.0513 - accuracy: 0.6619\n",
      "Epoch 85/200\n",
      "28/28 [==============================] - 0s 979us/step - loss: 1.1413 - accuracy: 0.6547\n",
      "Epoch 86/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.0867 - accuracy: 0.6835\n",
      "Epoch 87/200\n",
      "28/28 [==============================] - 0s 965us/step - loss: 1.0769 - accuracy: 0.6115\n",
      "Epoch 88/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 1.0669 - accuracy: 0.6331\n",
      "Epoch 89/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 1.0210 - accuracy: 0.6763\n",
      "Epoch 90/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.0580 - accuracy: 0.6978\n",
      "Epoch 91/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 1.0491 - accuracy: 0.6547\n",
      "Epoch 92/200\n",
      "28/28 [==============================] - 0s 963us/step - loss: 1.0586 - accuracy: 0.6619\n",
      "Epoch 93/200\n",
      "28/28 [==============================] - 0s 934us/step - loss: 1.0376 - accuracy: 0.6619\n",
      "Epoch 94/200\n",
      "28/28 [==============================] - 0s 963us/step - loss: 0.9806 - accuracy: 0.6835\n",
      "Epoch 95/200\n",
      "28/28 [==============================] - 0s 963us/step - loss: 0.9891 - accuracy: 0.6906\n",
      "Epoch 96/200\n",
      "28/28 [==============================] - 0s 945us/step - loss: 0.8839 - accuracy: 0.7842\n",
      "Epoch 97/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.8986 - accuracy: 0.7122\n",
      "Epoch 98/200\n",
      "28/28 [==============================] - 0s 963us/step - loss: 0.9435 - accuracy: 0.7338\n",
      "Epoch 99/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.9757 - accuracy: 0.6547\n",
      "Epoch 100/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.9369 - accuracy: 0.7194\n",
      "Epoch 101/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.8172 - accuracy: 0.7626\n",
      "Epoch 102/200\n",
      "28/28 [==============================] - 0s 963us/step - loss: 0.8633 - accuracy: 0.7554\n",
      "Epoch 103/200\n",
      "28/28 [==============================] - 0s 945us/step - loss: 0.8377 - accuracy: 0.7554\n",
      "Epoch 104/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.8973 - accuracy: 0.7194\n",
      "Epoch 105/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.8734 - accuracy: 0.7122\n",
      "Epoch 106/200\n",
      "28/28 [==============================] - 0s 983us/step - loss: 0.8595 - accuracy: 0.7266\n",
      "Epoch 107/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.8147 - accuracy: 0.7842\n",
      "Epoch 108/200\n",
      "28/28 [==============================] - 0s 963us/step - loss: 0.7949 - accuracy: 0.7986\n",
      "Epoch 109/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.8199 - accuracy: 0.7266\n",
      "Epoch 110/200\n",
      "28/28 [==============================] - 0s 974us/step - loss: 0.7083 - accuracy: 0.7986\n",
      "Epoch 111/200\n",
      "28/28 [==============================] - 0s 963us/step - loss: 0.7081 - accuracy: 0.7770\n",
      "Epoch 112/200\n",
      "28/28 [==============================] - 0s 934us/step - loss: 0.7881 - accuracy: 0.7842\n",
      "Epoch 113/200\n",
      "28/28 [==============================] - 0s 945us/step - loss: 0.6819 - accuracy: 0.7842\n",
      "Epoch 114/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.7023 - accuracy: 0.7842\n",
      "Epoch 115/200\n",
      "28/28 [==============================] - 0s 924us/step - loss: 0.6922 - accuracy: 0.7914\n",
      "Epoch 116/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.6651 - accuracy: 0.8201\n",
      "Epoch 117/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.6580 - accuracy: 0.8058\n",
      "Epoch 118/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.6195 - accuracy: 0.8201\n",
      "Epoch 119/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.6945 - accuracy: 0.8129\n",
      "Epoch 120/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.6637 - accuracy: 0.8273\n",
      "Epoch 121/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.6237 - accuracy: 0.8561\n",
      "Epoch 122/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.6633 - accuracy: 0.8561\n",
      "Epoch 123/200\n",
      "28/28 [==============================] - 0s 852us/step - loss: 0.6645 - accuracy: 0.8345\n",
      "Epoch 124/200\n",
      "28/28 [==============================] - 0s 909us/step - loss: 0.6063 - accuracy: 0.8345\n",
      "Epoch 125/200\n",
      "28/28 [==============================] - 0s 924us/step - loss: 0.6376 - accuracy: 0.8273\n",
      "Epoch 126/200\n",
      "28/28 [==============================] - 0s 908us/step - loss: 0.5797 - accuracy: 0.8201\n",
      "Epoch 127/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.5914 - accuracy: 0.8417\n",
      "Epoch 128/200\n",
      "28/28 [==============================] - 0s 936us/step - loss: 0.6486 - accuracy: 0.7698\n",
      "Epoch 129/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.5593 - accuracy: 0.8273\n",
      "Epoch 130/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.5169 - accuracy: 0.8849\n",
      "Epoch 131/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.5130 - accuracy: 0.8345\n",
      "Epoch 132/200\n",
      "28/28 [==============================] - 0s 871us/step - loss: 0.6237 - accuracy: 0.7914\n",
      "Epoch 133/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.4882 - accuracy: 0.8777\n",
      "Epoch 134/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.6119 - accuracy: 0.8129\n",
      "Epoch 135/200\n",
      "28/28 [==============================] - 0s 908us/step - loss: 0.5077 - accuracy: 0.8561\n",
      "Epoch 136/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.4907 - accuracy: 0.8633\n",
      "Epoch 137/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.5251 - accuracy: 0.8849\n",
      "Epoch 138/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.4814 - accuracy: 0.8705\n",
      "Epoch 139/200\n",
      "28/28 [==============================] - 0s 908us/step - loss: 0.4868 - accuracy: 0.8705\n",
      "Epoch 140/200\n",
      "28/28 [==============================] - 0s 920us/step - loss: 0.4920 - accuracy: 0.8417\n",
      "Epoch 141/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.4507 - accuracy: 0.8849\n",
      "Epoch 142/200\n",
      "28/28 [==============================] - 0s 937us/step - loss: 0.3917 - accuracy: 0.8921\n",
      "Epoch 143/200\n",
      "28/28 [==============================] - 0s 871us/step - loss: 0.4860 - accuracy: 0.8561\n",
      "Epoch 144/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.4205 - accuracy: 0.8993\n",
      "Epoch 145/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.4273 - accuracy: 0.8705\n",
      "Epoch 146/200\n",
      "28/28 [==============================] - 0s 871us/step - loss: 0.4155 - accuracy: 0.8921\n",
      "Epoch 147/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.3811 - accuracy: 0.9065\n",
      "Epoch 148/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.4518 - accuracy: 0.8777\n",
      "Epoch 149/200\n",
      "28/28 [==============================] - 0s 852us/step - loss: 0.3473 - accuracy: 0.9137\n",
      "Epoch 150/200\n",
      "28/28 [==============================] - 0s 908us/step - loss: 0.4620 - accuracy: 0.8921\n",
      "Epoch 151/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.4095 - accuracy: 0.8777\n",
      "Epoch 152/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.4268 - accuracy: 0.8705\n",
      "Epoch 153/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.4433 - accuracy: 0.8561\n",
      "Epoch 154/200\n",
      "28/28 [==============================] - 0s 908us/step - loss: 0.3564 - accuracy: 0.8993\n",
      "Epoch 155/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3592 - accuracy: 0.9137\n",
      "Epoch 156/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.4014 - accuracy: 0.8417\n",
      "Epoch 157/200\n",
      "28/28 [==============================] - 0s 908us/step - loss: 0.4378 - accuracy: 0.8489\n",
      "Epoch 158/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3831 - accuracy: 0.8921\n",
      "Epoch 159/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.3546 - accuracy: 0.8921\n",
      "Epoch 160/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.4243 - accuracy: 0.8705\n",
      "Epoch 161/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 891us/step - loss: 0.4000 - accuracy: 0.9137\n",
      "Epoch 162/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.3517 - accuracy: 0.8705\n",
      "Epoch 163/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.3954 - accuracy: 0.9065\n",
      "Epoch 164/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.4179 - accuracy: 0.9065\n",
      "Epoch 165/200\n",
      "28/28 [==============================] - 0s 883us/step - loss: 0.3481 - accuracy: 0.8921\n",
      "Epoch 166/200\n",
      "28/28 [==============================] - 0s 852us/step - loss: 0.4024 - accuracy: 0.8777\n",
      "Epoch 167/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.3594 - accuracy: 0.8921\n",
      "Epoch 168/200\n",
      "28/28 [==============================] - 0s 945us/step - loss: 0.3217 - accuracy: 0.9137\n",
      "Epoch 169/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.3501 - accuracy: 0.9065\n",
      "Epoch 170/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.3265 - accuracy: 0.9137\n",
      "Epoch 171/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.3606 - accuracy: 0.9281\n",
      "Epoch 172/200\n",
      "28/28 [==============================] - 0s 937us/step - loss: 0.3140 - accuracy: 0.9281\n",
      "Epoch 173/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.3218 - accuracy: 0.9281\n",
      "Epoch 174/200\n",
      "28/28 [==============================] - 0s 898us/step - loss: 0.2961 - accuracy: 0.9496\n",
      "Epoch 175/200\n",
      "28/28 [==============================] - 0s 908us/step - loss: 0.3117 - accuracy: 0.9353\n",
      "Epoch 176/200\n",
      "28/28 [==============================] - 0s 890us/step - loss: 0.3173 - accuracy: 0.9137\n",
      "Epoch 177/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.2620 - accuracy: 0.9353\n",
      "Epoch 178/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.2340 - accuracy: 0.9496\n",
      "Epoch 179/200\n",
      "28/28 [==============================] - 0s 941us/step - loss: 0.2578 - accuracy: 0.9424\n",
      "Epoch 180/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.2537 - accuracy: 0.9353\n",
      "Epoch 181/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.2655 - accuracy: 0.9353\n",
      "Epoch 182/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.3144 - accuracy: 0.9065\n",
      "Epoch 183/200\n",
      "28/28 [==============================] - 0s 871us/step - loss: 0.2118 - accuracy: 0.9424\n",
      "Epoch 184/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.2694 - accuracy: 0.9281\n",
      "Epoch 185/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.2683 - accuracy: 0.9496\n",
      "Epoch 186/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.2354 - accuracy: 0.9568\n",
      "Epoch 187/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.2729 - accuracy: 0.9137\n",
      "Epoch 188/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.2477 - accuracy: 0.9209\n",
      "Epoch 189/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.2709 - accuracy: 0.9353\n",
      "Epoch 190/200\n",
      "28/28 [==============================] - 0s 908us/step - loss: 0.2114 - accuracy: 0.9568\n",
      "Epoch 191/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.2499 - accuracy: 0.9281\n",
      "Epoch 192/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2914 - accuracy: 0.9281\n",
      "Epoch 193/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.2110 - accuracy: 0.9281\n",
      "Epoch 194/200\n",
      "28/28 [==============================] - 0s 871us/step - loss: 0.2751 - accuracy: 0.9353\n",
      "Epoch 195/200\n",
      "28/28 [==============================] - 0s 915us/step - loss: 0.2138 - accuracy: 0.9496\n",
      "Epoch 196/200\n",
      "28/28 [==============================] - 0s 989us/step - loss: 0.1937 - accuracy: 0.9496\n",
      "Epoch 197/200\n",
      "28/28 [==============================] - 0s 970us/step - loss: 0.2641 - accuracy: 0.9209\n",
      "Epoch 198/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.2320 - accuracy: 0.9209\n",
      "Epoch 199/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.1987 - accuracy: 0.9568\n",
      "Epoch 200/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.2366 - accuracy: 0.9281\n"
     ]
    }
   ],
   "source": [
    "# train the model by fitting the training data\n",
    "history=model.fit(np.array(X_train), np.array(y_train), epochs=200, verbose=1, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "25ba6e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_sentence (sentence):\n",
    "  sentence_words = nltk.word_tokenize(sentence)\n",
    "  sentence_words = [wnl.lemmatize(word) for word in sentence_words]\n",
    "  return sentence_words\n",
    "\n",
    "\n",
    "def bag_of_words (sentence):\n",
    "  sentence_words = clean_up_sentence (sentence)\n",
    "  bag = [0] * len (corps_voc)\n",
    "  for w in sentence_words:\n",
    "    for i, word in enumerate(corps_voc) :\n",
    "      if word == w:\n",
    "        bag[i] = 1\n",
    "  return np.array (bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8ec3a85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_response(sentence):\n",
    "  bow = bag_of_words (sentence.lower())\n",
    "  res = model.predict(np.array([bow]), verbose=0)[0]\n",
    "  ERROR_THRESHOLD = 0.25\n",
    "  results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]\n",
    "  results.sort(key=lambda x: x[1], reverse=True)\n",
    "  return_list= []\n",
    "  for r in results:\n",
    "    return_list.append({'intent': corps_tags[r[0]], 'probability' : str(r[1])})\n",
    "  return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7d2b37d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'intent': 'GL', 'probability': '0.8038906'}]\n"
     ]
    }
   ],
   "source": [
    "print(predict_response('AIML'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e07787e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot Running\n",
      "Welcome , Hi I am the Chatbot \n"
     ]
    }
   ],
   "source": [
    "def get_response(intents_list, intents_json):\n",
    "  tag = intents_list[0]['intent']\n",
    "  list_of_intents = intents_json['intents']\n",
    "  result = 'I cannot understand,  please re-phrase the question'\n",
    "  for i in list_of_intents:\n",
    "    if i['tag'] == tag:\n",
    "      result = random.choice(i['responses'])\n",
    "      break\n",
    "  return result\n",
    "\n",
    "print (\"Bot Running\")\n",
    "print(\"Welcome , Hi I am the Chatbot \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1195d369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am from\n",
      "Hello! how can i help you ?\n",
      "online\n",
      "Hello! how can i help you ?\n",
      "not able to understand\n",
      "Link: Machine Learning wiki \n",
      "good bye\n",
      "I hope I was able to assist you, Good Bye\n",
      "Good bye\n"
     ]
    }
   ],
   "source": [
    "exit_intent = 'Exit'\n",
    "chat_alive = True\n",
    "while chat_alive:\n",
    "  message = input(\"\")\n",
    "  ints = predict_response(message.lower())\n",
    "  if(ints[0]['intent'] == exit_intent):\n",
    "    chat_alive = False\n",
    "  res = get_response (ints, json_data)\n",
    "  print(res)\n",
    "\n",
    "print('Good bye')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c524c178",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
